{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets start by getting the data\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import urllib2\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "In this notebook we gather additional data to support our analysis of the Kaggle movie dataset. The additional data is the following\n",
    "\n",
    "- **IMDb rating for each movie in the dataset**\n",
    "- **IMDb storyline for each movie in the dataset**\n",
    "- ** Download all manuscripts from the www.imsdb.com website, 1116 movie manuscripts in total. **\n",
    "\n",
    "\n",
    "\n",
    "**Note** We also downloaded the movie reviews from the website http://ai.stanford.edu/~amaas/data/sentiment/. The code as well as analysis of the movie reviews can be found in the notebook 'Sentiment_reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These are movies we couldn't find the IMDb rating for or they were missing crucial data in the Kaggle dataset\n",
    "movies_to_delete =[\n",
    "    'National Lampoon’s Van Wilder',\n",
    "    'Bran Nue Dae',\n",
    "    'Pokémon: Spell of the Unknown',\n",
    "    'Alien³',\n",
    "    'Guten Tag, Ramón'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get data line by line from tmdb-5000-movie-database and save the data as dictionary\n",
    "\n",
    "filepath = \"/Users/GretarAtli/Documents/GitHub/Dtu/Dtu-SocialGraphs-FinalProject/Data/tmdb-5000-movie-dataset/tmdb_5000_movies.csv\"\n",
    "tmdb_5000_movies = defaultdict(dict)\n",
    "\n",
    "with open(filepath) as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=\",\")\n",
    "    for row in reader:\n",
    "        tmdb_5000_movies[row[\"title\"]] = row\n",
    "        \n",
    "for title in sorted(tmdb_5000_movies.keys()):\n",
    "    title = title.replace(\" \",\"-\").replace(\"(\",\"\").replace(\")\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get data line by line from tmdb-5000-movie-database and save the data as dictionary\n",
    "filepath = \"/Users/GretarAtli/Documents/GitHub/Dtu/Dtu-SocialGraphs-FinalProject/Data/tmdb-5000-movie-dataset/tmdb_5000_credits.csv\"\n",
    "tmdb_5000_credits = defaultdict(dict)\n",
    "\n",
    "with open(filepath) as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=\",\")\n",
    "    for row in reader:\n",
    "        tmdb_5000_credits[row[\"title\"]] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Scrape the IMDb website for IMDb ratings\n",
    "\n",
    "The following code uses the BeautifulSoup module in python to get the IMDb rating for each movie in our Kaggle dataset. The code scrapes the [IMDb](http://www.imdb.com/) website and collects the rating for each movie and saves it to a dictionary. We then save the dictionary as a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import urllib\n",
    "import urlparse\n",
    " \n",
    "from mechanize import Browser\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "# title of the movie\n",
    "title = \"Avatar\"\n",
    "\n",
    "def getImdbRatings(title):\n",
    "\n",
    "    # IMDB URL of the movie\n",
    "    url = None\n",
    "    # IMDB rating of the movie\n",
    "    rating = None\n",
    "    # Did we find a result?\n",
    "    found = False\n",
    "\n",
    "    # constant\n",
    "    BASE_URL = 'http://www.imdb.com'\n",
    "\n",
    "    movie = '+'.join(title.split())\n",
    "    br = Browser()\n",
    "    url = \"%s/find?s=tt&q=%s\" % (BASE_URL, movie)\n",
    "        \n",
    "    try:\n",
    "        br.open(url)\n",
    "    \n",
    "        if re.search(r'/title/tt.*', br.geturl()):\n",
    "            url = \"%s://%s%s\" % urlparse.urlparse(br.geturl())[:3]\n",
    "            print url\n",
    "            soup = BeautifulSoup( html_doc, 'html.parser' )\n",
    "        else:\n",
    "            link = br.find_link(url_regex = re.compile(r'/title/tt.*'))\n",
    "            res = br.follow_link(link)\n",
    "            url = urlparse.urljoin(BASE_URL, link.url)\n",
    "            print url\n",
    "            soup = BeautifulSoup(res.read())\n",
    "\n",
    "        title = soup.find('h1').contents[0].strip()\n",
    "        for span in soup.findAll('span'):\n",
    "            if span.has_key('itemprop') and span['itemprop'] == 'ratingValue':\n",
    "                rating = span.contents[0]\n",
    "                break\n",
    "        found = True\n",
    "        return rating\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "Imdb_5000_movies = defaultdict(float)\n",
    "\n",
    "for i,title in enumerate(sorted(tmdb_5000_movies.keys())):\n",
    "    title_to_use = title.replace(\"(\",\"\").replace(\")\",\"\")\n",
    "    print title_to_use\n",
    "    imdb_rating = getImdbRatings(title)\n",
    "    print imdb_rating\n",
    "    Imdb_5000_movies[tmdb_5000_movies[title]['id']] = imdb_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save IMDb ratings to file**\n",
    "\n",
    "Adding the IMDb values manually that failed when scraping the imdb website. There where some movies that failed when we tried to scrape the IMDb website. These movies where added manually, if IMDb did not have any information about the movie then we simply assign the value 'None' to those movies and then in the analysis we ignore these movies. These movies are only 5 and therefore we did not consider this to be a major problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Imdb_5000_movies_withid[\"161795\"] = '7.2'\n",
    "Imdb_5000_movies_withid[\"8859\"] = '5.5'\n",
    "Imdb_5000_movies_withid[\"11452\"] = 'None'\n",
    "Imdb_5000_movies_withid[\"10681\"] = '8.4'\n",
    "Imdb_5000_movies_withid[\"10991\"] = 'None'\n",
    "Imdb_5000_movies_withid[\"37137\"] = '6.8'\n",
    "Imdb_5000_movies_withid[\"18480\"] = 'None'\n",
    "Imdb_5000_movies_withid[\"8077\"] = 'None'\n",
    "Imdb_5000_movies_withid[\"10664\"] = '7.5'\n",
    "Imdb_5000_movies_withid[\"11661\"] = '7.8'\n",
    "Imdb_5000_movies_withid[\"82695\"] = '7.6'\n",
    "Imdb_5000_movies_withid[\"333355\"] = '8.0'\n",
    "Imdb_5000_movies_withid[\"41009\"] = '6.4'\n",
    "Imdb_5000_movies_withid[\"1391\"] = '7.7'\n",
    "Imdb_5000_movies_withid[\"242575\"] = 'None'\n",
    "Imdb_5000_movies_withid[\"36593\"] = '6.5'\n",
    "Imdb_5000_movies_withid[\"335244\"] = '6.9'\n",
    "Imdb_5000_movies_withid[\"64499\"] = '7.7'\n",
    "Imdb_5000_movies_withid[\"38570\"] = '2.6'\n",
    "Imdb_5000_movies_withid[\"301325\"] = '3.1'\n",
    "Imdb_5000_movies_withid[\"194\"] = '8.3'\n",
    "Imdb_5000_movies_withid[\"304410\"] = '7.4'\n",
    "Imdb_5000_movies_withid[\"11011\"] = '5.3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the result as a json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "result_file_uri = \"/Users/GretarAtli/Dropbox/SocialGraph/results/imdb-score-mod.json\"\n",
    "\n",
    "#This is commented out as to not repeat\n",
    "#with open(result_file_uri, 'w') as fp:\n",
    "#    json.dump(Imdb_5000_movies_withid, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4797"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Imdb_5000_movies_withid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Get IMDb rating from file **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use the fact that we know that each movie has a name, so we do not include any error handling\n",
    "def findMovieNameFromId(id):\n",
    "    name = [key for key,info in tmdb_5000_movies.items() if info['id'] == id]\n",
    "    return name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_file_uri = \"/Users/GretarAtli/Dropbox/SocialGraph/results/imdb-score-mod.json\"\n",
    "\n",
    "# Getting Imdb scores from data and convert the id to name \n",
    "json1_file = open(result_file_uri)\n",
    "json1_str = json1_file.read()\n",
    "Imdb_5000_movies_withid = json.loads(json1_str)\n",
    "\n",
    "Imdb_5000_movies = defaultdict(float)\n",
    "\n",
    "# change the dictionary key from id to name of the movie\n",
    "for key,value in Imdb_5000_movies_withid.items():\n",
    "    try:\n",
    "        name = findMovieNameFromId(key)\n",
    "        Imdb_5000_movies[name] = (value)\n",
    "    except: \n",
    "        print(\"The movie {} has been removed from kaggle database\".format(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the IMDb website for IMDb story line\n",
    "\n",
    "The following code uses the BeautifulSoup module in python to get the IMDb storyline for each movie in our Kaggle database. The code scrapes the [IMDb](http://www.imdb.com/) website and collects the storyline for each movie and saves it to a dictionary. We then save the dictionary as a json file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import urllib\n",
    "import urlparse\n",
    " \n",
    "from mechanize import Browser\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "# title of the movie\n",
    "title = \"Avatar\"\n",
    "\n",
    "def getImdbStoryLine(title):\n",
    "\n",
    "    # IMDB URL of the movie\n",
    "    url = None\n",
    "    # IMDB rating of the movie\n",
    "    rating = None\n",
    "    # Did we find a result?\n",
    "    found = False\n",
    "\n",
    "    # constant\n",
    "    BASE_URL = 'http://www.imdb.com'\n",
    "\n",
    "    movie = '+'.join(title.split())\n",
    "    br = Browser()\n",
    "    url = \"%s/find?s=tt&q=%s\" % (BASE_URL, movie)\n",
    "        \n",
    "    try:\n",
    "        br.open(url)\n",
    "    \n",
    "        if re.search(r'/title/tt.*', br.geturl()):\n",
    "            url = \"%s://%s%s\" % urlparse.urlparse(br.geturl())[:3]\n",
    "            print url\n",
    "            soup = BeautifulSoup( html_doc, 'html.parser' )\n",
    "        else:\n",
    "            link = br.find_link(url_regex = re.compile(r'/title/tt.*'))\n",
    "            res = br.follow_link(link)\n",
    "            url = urlparse.urljoin(BASE_URL, link.url)\n",
    "            print url\n",
    "            soup = BeautifulSoup(res.read())\n",
    "\n",
    "        title = soup.find('h1').contents[0].strip()\n",
    "        \n",
    "        for i in soup.findAll('div',{\"class\":\"inline canwrap\",\"itemprop\":\"description\"}):\n",
    "            p = i.find(\"p\")\n",
    "            return p.text\n",
    "\n",
    "        found = True\n",
    "        return rating\n",
    "    except Exception as e:\n",
    "        print e\n",
    "        return 0\n",
    "\n",
    "Imdb_5000_storyline = defaultdict(str)\n",
    "\n",
    "for i,title in enumerate(sorted(tmdb_5000_movies.keys())):\n",
    "    title_to_use = title.replace(\"(\",\"\").replace(\")\",\"\")\n",
    "    print title_to_use\n",
    "    imdb_storyline = getImdbStoryLine(title)\n",
    "    #print imdb_storyline\n",
    "    Imdb_5000_storyline[tmdb_5000_movies[title]['id']] = imdb_storyline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Save IMDb storyline to file **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "result_file_uri_story = \"/Users/GretarAtli/Dropbox/SocialGraph/results/imdb-storyline.json\"\n",
    "\n",
    "with open(result_file_uri_story, 'w') as fp:\n",
    "    json.dump(Imdb_5000_storyline, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all manuscripts from imsdb.com\n",
    "\n",
    "This is a python code that downloads (and cleans) all \n",
    "scripts on the website [IMSDb](http://www.imsdb.com).\n",
    "\n",
    "This code was taken from the [this github repository](https://github.com/j2kun/imsdb_download_all_scripts) and adapted to our needs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import quote\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "BASE_URL = 'http://www.imsdb.com'\n",
    "SCRIPTS_DIR = 'scripts'\n",
    "\n",
    "\n",
    "def clean_script(text):\n",
    "    text = text.replace('Back to IMSDb', '')\n",
    "    text = text.replace('''<b><!--\n",
    "</b>if (window!= top)\n",
    "top.location.href=location.href\n",
    "<b>// -->\n",
    "</b>\n",
    "''', '')\n",
    "    text = text.replace('''          Scanned by http://freemoviescripts.com\n",
    "          Formatting by http://simplyscripts.home.att.net\n",
    "''', '')\n",
    "    return text.replace(r'\\r', '')\n",
    "\n",
    "\n",
    "def get_script(relative_link):\n",
    "    tail = relative_link.split('/')[-1]\n",
    "    print('fetching %s' % tail)\n",
    "    script_front_url = BASE_URL + quote(relative_link)\n",
    "    front_page_response = requests.get(script_front_url)\n",
    "    front_soup = BeautifulSoup(front_page_response.text, \"html.parser\")\n",
    "\n",
    "    try:\n",
    "        script_link = front_soup.find_all('p', align=\"center\")[0].a['href']\n",
    "    except IndexError:\n",
    "        print('%s has no script :(' % tail)\n",
    "        return None, None\n",
    "\n",
    "    if script_link.endswith('.html'):\n",
    "        title = script_link.split('/')[-1].split(' Script')[0]\n",
    "        script_url = BASE_URL + script_link\n",
    "        script_soup = BeautifulSoup(requests.get(script_url).text, \"html.parser\")\n",
    "        script_text = script_soup.find_all('td', {'class': \"scrtext\"})[0].get_text()\n",
    "        script_text = clean_script(script_text)\n",
    "        return title, script_text\n",
    "    else:\n",
    "        print('%s is a pdf :(' % tail)\n",
    "        return None, None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    response = requests.get('http://www.imsdb.com/all%20scripts/')\n",
    "    html = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    paragraphs = soup.find_all('p')\n",
    "\n",
    "    for p in paragraphs:\n",
    "        relative_link = p.a['href']\n",
    "        title, script = get_script(relative_link)\n",
    "        if not script:\n",
    "            continue\n",
    "\n",
    "        with open(os.path.join(SCRIPTS_DIR, title.strip('.html') + '.txt'), 'w') as outfile:\n",
    "            outfile.write(script)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
